from datasets import load_dataset
import questionary
import os


def save_list(path, words):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ª–æ–≤–∞ –≤ —Ñ–∞–π–ª"""
    with open(path, "w", encoding="utf-8") as f:
        for w in words:
            f.write(w + "\n")
    print(f"‚úî –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}  (—Å–ª–æ–≤: {len(words)})")


def main():
    print("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç textdetox/multilingual_toxic_lexicon...")
    dataset = load_dataset("textdetox/multilingual_toxic_lexicon")

    # –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–∑—ã–∫–∏ (–∫–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è)
    languages = list(dataset.keys())

    lang = questionary.select(
        "–ö–∞–∫–æ–π —è–∑—ã–∫ —Å–∫–∞—á–∞—Ç—å?",
        choices=languages
    ).ask()

    print(f"üìå –í—ã–±—Ä–∞–Ω —è–∑—ã–∫: {lang}")

    data = dataset[lang]

    # –í –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–ª–æ–≤–∞ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –∫–æ–ª–æ–Ω–∫–µ "text"
    words = list(set(data["text"]))  # —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ

    os.makedirs("toxic_wordlists", exist_ok=True)
    output_path = f"toxic_wordlists/toxic_words_{lang}.txt"

    save_list(output_path, words)

    print("\nüéâ –ì–æ—Ç–æ–≤–æ!")
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(words)}")


if __name__ == "__main__":
    main()
